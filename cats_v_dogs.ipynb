{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "TEST_PERCENT = 0.1\n",
    "SELECT_SUBSET_PERCENT = 0.9\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "# DO NOT CHANGE\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "# Setting to 5 epochs for it to run without GPU \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 22500 files\n",
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Have loaded 7000 samples\n",
      "Have loaded 8000 samples\n",
      "Have loaded 9000 samples\n",
      "Have loaded 10000 samples\n",
      "Have loaded 11000 samples\n",
      "Have loaded 12000 samples\n",
      "Have loaded 13000 samples\n",
      "Have loaded 14000 samples\n",
      "Have loaded 15000 samples\n",
      "Have loaded 16000 samples\n",
      "Have loaded 17000 samples\n",
      "Have loaded 18000 samples\n",
      "Have loaded 19000 samples\n",
      "Have loaded 20000 samples\n",
      "Have loaded 21000 samples\n",
      "Have loaded 22000 samples\n",
      "Train set has dimensionality (20250, 32, 32, 3)\n",
      "Test set has dimensionality (2250, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading the data.\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary layers.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Define the network\n",
    "model.add(Conv2D(600, input_shape=(32, 32, 3), kernel_size = 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(300, kernel_size = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(150, kernel_size = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Define your loss and your objective\n",
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16200 samples, validate on 4050 samples\n",
      "Epoch 1/5\n",
      "16200/16200 [==============================] - 390s - loss: 0.6719 - acc: 0.5788 - val_loss: 0.6185 - val_acc: 0.6768\n",
      "Epoch 2/5\n",
      "16200/16200 [==============================] - 353s - loss: 0.6210 - acc: 0.6540 - val_loss: 0.6229 - val_acc: 0.6496\n",
      "Epoch 3/5\n",
      "16200/16200 [==============================] - 325s - loss: 0.5921 - acc: 0.6852 - val_loss: 0.6113 - val_acc: 0.6546\n",
      "Epoch 4/5\n",
      "16200/16200 [==============================] - 375s - loss: 0.5623 - acc: 0.7102 - val_loss: 0.5206 - val_acc: 0.7449\n",
      "Epoch 5/5\n",
      "16200/16200 [==============================] - 388s - loss: 0.5388 - acc: 0.7246 - val_loss: 0.5180 - val_acc: 0.7437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f9366a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 9s     \n",
      "\n",
      "Got 73.56% accuracy\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
